---
---

@string{aps = {American Physical Society,}}

@inproceedings{pigeon,
      title={PIGEON: Optimizing CUDA Code Generator for End-to-End Training and Inference of Relational Graph Neural Networks}, 
      author={Kun Wu and Mert Hidayetoğlu and Xiang Song and Sitao Huang and Da Zheng and Israt Nisa and Wen-mei Hwu},
      year={2022},
      booktitle={arXiv preprint releasing this Wednesday},
      primaryClass={cs.DC},
      selected={true}
}

@inproceedings{min2021graph,
      title={Graph Neural Network Training with Data Tiering}, 
      booktitle={SIGKDD International Conference on Knowledge Discovery and Data Mining},
      author={Seung Won Min and Kun Wu and Mert Hidayetoğlu and Jinjun Xiong and Xiang Song and Wen-mei Hwu},
      year={2022},
      abstract={Graph Neural Networks (GNNs) have shown success in learning from graph-structured data, with applications to fraud detection, recommendation, and knowledge graph reasoning. However, training GNN efficiently is challenging because: 1) GPU memory capacity is limited and can be insufficient for large datasets, and 2) the graph-based data structure causes irregular data access patterns. In this work, we provide a method to statistically analyze and identify more frequently accessed data ahead of GNN training. Our data tiering method not only utilizes the structure of input graph, but also an insight gained from actual GNN training process to achieve a higher prediction result. With our data tiering method, we additionally provide a new data placement and access strategy to further minimize the CPU-GPU communication overhead. We also take into account of multi-GPU GNN training as well and we demonstrate the effectiveness of our strategy in a multi-GPU system. The evaluation results show that our work reduces CPU-GPU traffic by 87–95% and improves the training speed of GNN over the existing solutions by 1.6–2.1× on graphs with hundreds of millions of nodes and billions of edges.},
      paperlink={https://www.amazon.science/publications/graph-neural-network-training-with-data-tiering},
      selected={true}
}

@inproceedings{HPETechCon22,
author = {Wu*, Kun and Korolija*, Dario and Hwu,Wen-mei and Alonso, Gustavo and Chalamalasetti, Sai Rahul and Milojicic, Dejan and Evans, Lance},
title = {SaintSN: Streamlined and Intelligent Storage Node System-on-a-Chip for Exascale Cluster},
year = {2022},
booktitle = {Proceedings of the Hewlett Packard Enterprise Technical Conference},
my_note = {Acceptance rate: 17.6. Pending US patent.},
series = {HPE TechCon '22},
selected={true}
}

@ARTICLE{9591456,
  author={Huang, Sitao and Wu, Kun and Jeong, Hyunmin and Wang, Chengyue and Chen, Deming and Hwu, Wen-Mei},
  journal={IEEE Transactions on Computers}, 
  title={PyLog: An Algorithm-Centric Python-Based FPGA Programming and Synthesis Flow}, 
  paperlink={https://ieeexplore.ieee.org/abstract/document/9591456},
  year={2021},
  volume={70},
  number={12},
  pages={2015-2028},
  doi={10.1109/TC.2021.3123465},
  codelink={https://github.com/hst10/pylog},
  abstract={The exploding complexity and computation efficiency requirements of applications are stimulating a strong demand for hardware acceleration with heterogeneous platforms such as FPGAs. However, a high-quality FPGA design is very hard to create and optimize as it requires FPGA expertise and a long design iteration time. In contrast, software applications are typically developed in a short development cycle, with high-level languages like Python, which have much higher levels of abstraction than all existing hardware design flows. To close this gap between hardware design flows and software applications, and simplify FPGA programming, we create PyLog, a high-level, algorithm-centric Python-based programming and synthesis flow for FPGA. PyLog is powered by a set of compiler optimization passes and a type inference system to generate high-quality hardware design. It abstracts away the implementation details, and allows designers to focus on algorithm specification. PyLog takes in Python functions, generates PyLog intermediate representation (PyLog IR), performs several optimization passes, including pragma insertion, design space exploration, and memory customization, etc., and creates complete FPGA system designs. PyLog also has a runtime that allows users to run the PyLog code directly on the target FPGA platform without any extra code development. The whole design flow is automated. Evaluation shows that PyLog significantly improves FPGA design productivity and generates highly efficient FPGA designs that outperform highly optimized CPU implementation and state-of-the-art FPGA implementation by 3.17× and 1.24× on average.},
  selected={true}}


@article{10.14778/3476249.3476264,
author = {Min, Seung Won and Wu, Kun and Huang, Sitao and Hidayeto\u{g}lu, Mert and Xiong, Jinjun and Ebrahimi, Eiman and Chen, Deming and Hwu, Wen-mei},
title = {Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476264},
paperlink = {https://doi.org/10.14778/3476249.3476264},
my_note = {upstreamed to DGL},
doi = {10.14778/3476249.3476264},
abstract = {Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale graph-based recommender systems. Training GCN requires the minibatch generator traversing graphs and sampling the sparsely located neighboring nodes to obtain their features. Since real-world graphs often exceed the capacity of GPU memory, current GCN training systems keep the feature table in host memory and rely on the CPU to collect sparse features before sending them to the GPUs. This approach, however, puts tremendous pressure on host memory bandwidth and the CPU. This is because the CPU needs to (1) read sparse features from memory, (2) write features into memory as a dense format, and (3) transfer the features from memory to the GPUs.In this work, we propose a novel GPU-oriented data communication approach for GCN training, where GPU threads directly access sparse features in host memory through zero-copy accesses without much CPU help. By removing the CPU gathering stage, our method significantly reduces the consumption of the host resources and data access latency. We further present two important techniques to achieve high host memory access efficiency by the GPU: (1) automatic data access address alignment to maximize PCIe packet efficiency, and (2) asynchronous zero-copy access and kernel execution to fully overlap data transfer with training. We incorporate our method into PyTorch and evaluate its effectiveness using several graphs with sizes up to 111 million nodes and 1.6 billion edges. In a multi-GPU training setup, our method is 65--92% faster than the conventional data transfer method, and can even match the performance of all-in-GPU-memory training for some graphs that fit in GPU memory.},
journal = {Proceedings of the VLDB Endowment},
pages = {2087–2100},
numpages = {14},
codelink={https://github.com/K-Wu/pytorch-direct_dgl},
selected={true}
}

@inproceedings{min2021pytorchdirect,
      title={PyTorch-Direct: Enabling GPU Centric Data Access for Very Large Graph Neural Network Training with Irregular Accesses}, 
      author={Seung Won Min and Kun Wu and Sitao Huang and Mert Hidayetoğlu and Jinjun Xiong and Eiman Ebrahimi and Deming Chen and Wen-mei Hwu},
      year={2021},
      booktitle={arXiv preprint},
      arxiv={2101.07956},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      codelink={https://github.com/K-Wu/pytorch-direct_dgl},
      selected={true}
}

@INPROCEEDINGS{9651170,
  author={Huang, Sitao and Wu, Kun and Chalamalasetti, Sai Rahul and El Hajj, Izzat and Xu, Cong and Faraboschi, Paolo and Chen, Deming},
  booktitle={2021 IEEE/ACM Programming Environments for Heterogeneous Computing (PEHC)}, 
  title={A Python-based High-Level Programming Flow for CPU-FPGA Heterogeneous Systems : (Invited Paper)}, 
  year={2021},
  paperlink={https://ieeexplore.ieee.org/abstract/document/9651170},
  volume={},
  number={},
  pages={20-26},
  codelink={https://github.com/hst10/pylog},
  doi={10.1109/PEHC54839.2021.00008}}

@inproceedings{10.1145/3431379.3460645,
author = {Pearson, Carl and Wu, Kun and Chung, I-Hsin and Xiong, Jinjun and Hwu, Wen-Mei},
title = {TEMPI: An Interposed MPI Library with a Canonical Representation of CUDA-Aware Datatypes},
year = {2021},
isbn = {9781450382175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3431379.3460645},
paperlink = {https://doi.org/10.1145/3431379.3460645},
doi = {10.1145/3431379.3460645},
abstract = {MPI derived datatypes are an abstraction that simplifies handling of non-contiguous data in MPI applications. These datatypes are recursively constructed at runtime from primitive Named Types defined in the MPI standard. More recently, the development and deployment of CUDA-aware MPI implementations has encouraged the transition of distributed high-performance MPI codes to use GPUs. Such implementations allow MPI functions to directly operate on GPU buffers, easing integration of GPU compute into MPI codes. This work first presents a novel datatype handling strategy for nested strided datatypes, which finds a middle ground between the specialized or generic handling in prior work. This work also shows that the performance characteristics of non-contiguous data handling can be modeled with empirical system measurements, and used to transparently improve MPI_Send/Recv latency. Finally, despite substantial attention to non-contiguous GPU data and CUDA-aware MPI implementations, good performance cannot be taken for granted. This work demonstrates its contributions through an MPI interposer library, TEMPI. TEMPI can be used with existing MPI deployments without system or application changes. Ultimately, the interposed-library model of this work demonstrates MPI_Pack speedup of up to 242000x and MPI_Send speedup of up to 59000x compared to the MPI implementation deployed on a leadership-class supercomputer. This yields speedup of more than 917x in a 3D halo exchange with 3072 processes.},
booktitle = {Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {95–106},
numpages = {12},
keywords = {summit, cuda, derived datatype, spectrum mpi, mpi},
location = {Virtual Event, Sweden},
series = {HPDC '21},
codelink={https://github.com/cwpearson/tempi},
selected={true}
}

@inproceedings{10.1145/3316781.3317862,
author = {Wu, Kun and Dai, Guohao and Hu, Xing and Li, Shuangchen and Xie, Xinfeng and Wang, Yu and Xie, Yuan},
title = {Memory-Bound Proof-of-Work Acceleration for Blockchain Applications},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3317862},
doi = {10.1145/3316781.3317862},
abstract = {Blockchain applications have shown huge potential in various domains. Proof of Work (PoW) is the key procedure in blockchain applications, which exhibits the memory-bound characteristic and hinders the performance improvement of blockchain accelerators. In order to mitigate the "memory wall" and improve the performance of memory-hard PoW accelerators, using Ethash as an example, we optimize the memory architecture from two perspectives: 1) Hiding memory latency. We propose specialized context switch design to overcome the uncertain cycles of repetitive memory requests. 2) Increasing memory bandwidth utilization. We introduce on-chip memory that stores a portion of the Ethash directed acyclic graph (DAG) for larger effective memory bandwidth, and further propose adopting embedded NOR flash to fulfill the role. Then, we conduct extensive experiments to explore the design space of our optimized memory architecture for Ethash, including number of hash cores, on-chip/off-chip memory technologies and specifications. Based on the design space exploration, we finally provide the guidance for designing the memory-bound PoW accelerator. The experiment results show that our optimized designs achieve 8.7% -- 55% higher hash rate and 17% -- 120% higher hash rate per Joule compared with the baseline design in different configurations.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference},
articleno = {177},
numpages = {6},
location = {Las Vegas, NV, USA},
series = {DAC '19},
selected={true}
}