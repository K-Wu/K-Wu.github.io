---
---

@string{aps = {American Physical Society,}}

@inproceedings{wu2024fasterlargelanguage,
  title         = {TBA: Faster Large Language Model Training Using SSD-Based Activation Offloading},
  author        = {Kun Wu and Jeongmin Brian Park and Xiaofan Zhang and Mert Hidayetoğlu and Vikram Sharma Mailthody and Sitao Huang and Steven Sam Lumetta and Wen-mei Hwu},
  year          = {2024},
  booktitle     = {arXiv preprint},
  arxiv         = {2408.10013},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC},
  selected      = {true},
  abstract      = {The growth rate of the GPU memory capacity has not been able to keep up with that of the size of large language models (LLMs), hindering the model training process. In particular, activations---the intermediate tensors produced during forward propagation and reused in backward propagation---dominate the GPU memory use. To address this challenge, we propose TBA to efficiently offload activations to high-capacity NVMe SSDs. This approach reduces GPU memory usage without impacting performance by adaptively overlapping data transfers with computation. TBA is compatible with popular deep learning frameworks like PyTorch, Megatron, and DeepSpeed, and it employs techniques such as tensor deduplication, forwarding, and adaptive offloading to further enhance efficiency. We conduct extensive experiments on popular LLMs like GPT, BERT, and T5. Results demonstrate that TBA effectively reduces 47\% of the activation peak memory usage. At the same time, TBA perfectly overlaps the I/O with the computation and incurs negligible performance overhead. We introduce the recompute-offload-keep (ROK) curve to compare the TBA offloading with two other tensor placement strategies, keeping activations in GPU memory and layerwise full recomputation.  We find that TBA achieves better memory savings than layerwise full recomputation while retaining the performance of keeping the activations in memory.}
}

@inproceedings{wu2023hector,
  title     = {Hector: An Efficient Programming and Compilation Framework for Implementing Relational Graph Neural Networks in GPU Architectures},
  author    = {Kun Wu and Mert Hidayetoğlu and Xiang Song and Sitao Huang and Da Zheng and Israt Nisa and Wen-mei Hwu},
  year      = {2024},
  isbn      = {9798400703867},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3620666.3651322},
  paperlink = {https://arxiv.org/abs/2301.06284},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages     = {528–544},
  numpages  = {17},
  location  = {, La Jolla, CA, USA, },
  series    = {ASPLOS '24},
  abstract  = {Relational graph neural networks (RGNNs) are graph neural networks with dedicated structures for modeling the different types of nodes and edges in heterogeneous graphs. While RGNNs have been increasingly adopted in many real-world applications due to their versatility and accuracy, they pose performance and system design challenges: inherent memory-intensive computation patterns, the gap between the programming interface and kernel APIs, and heavy programming effort in optimizing kernels caused by their coupling with data layout and heterogeneity. To systematically address these challenges, we propose Hector, a novel two-level intermediate representation and its code generator framework, that (a) captures the key properties of RGNN models, and opportunities to reduce memory accesses in inter-operator scheduling and materialization, (b) generates code with flexible data access scheme to eliminate redundant data copies, (c) decouples model semantics, data layout, and operators-specific optimization from each other to reduce programming effort. By building on one general matrix multiply (GEMM) template and a node/edge traversal template, Hector achieves up to 9.9× speed-up in inference and 43.7× speed-up in training compared with the state-of-the-art public systems on select models, i.e., RGCN, RGAT and HGT, when running heterogeneous graphs provided by Deep Graph Library (DGL) and Open Graph Benchmark (OGB). In addition, Hector does not trigger any out-of-memory (OOM) exception in these tests. We also propose the linear operator reorder and compact materialization to further accelerate the system by up to 3.8×. As an indicator of programming effort reduction, Hector takes in 51 lines of code expressing the three models and generates a total of 8K lines of CUDA and C++ code. Through profiling, we found that higher memory efficiency allows Hector to accomodate larger input and, therefore, attains higher throughput in forward propagation, while backward propagation is bound by latency introduced by atomic updates and outer products.},
  selected  = {true}
}

@inproceedings{min2021graph,
  title     = {Graph Neural Network Training with Data Tiering},
  booktitle = {SIGKDD International Conference on Knowledge Discovery and Data Mining},
  author    = {Seung Won Min and Kun Wu and Mert Hidayetoğlu and Jinjun Xiong and Xiang Song and Wen-mei Hwu},
  year      = {2022},
  abstract  = {Graph Neural Networks (GNNs) have shown success in learning from graph-structured data, with applications to fraud detection, recommendation, and knowledge graph reasoning. However, training GNN efficiently is challenging because: 1) GPU memory capacity is limited and can be insufficient for large datasets, and 2) the graph-based data structure causes irregular data access patterns. In this work, we provide a method to statistically analyze and identify more frequently accessed data ahead of GNN training. Our data tiering method not only utilizes the structure of input graph, but also an insight gained from actual GNN training process to achieve a higher prediction result. With our data tiering method, we additionally provide a new data placement and access strategy to further minimize the CPU-GPU communication overhead. We also take into account of multi-GPU GNN training as well and we demonstrate the effectiveness of our strategy in a multi-GPU system. The evaluation results show that our work reduces CPU-GPU traffic by 87–95% and improves the training speed of GNN over the existing solutions by 1.6–2.1× on graphs with hundreds of millions of nodes and billions of edges.},
  paperlink = {https://www.amazon.science/publications/graph-neural-network-training-with-data-tiering},
  selected  = {true}
}

@inproceedings{HPETechCon22,
  author    = {Wu*, Kun and Korolija*, Dario and Hwu,Wen-mei and Alonso, Gustavo and Chalamalasetti, Sai Rahul and Milojicic, Dejan and Evans, Lance},
  title     = {SaintSN: Streamlined and Intelligent Storage Node System-on-a-Chip for Exascale Cluster},
  year      = {2022},
  booktitle = {Proceedings of the Hewlett Packard Enterprise Technical Conference},
  my_note   = {Acceptance rate: 17.6\%. Pending US patent.},
  series    = {HPE TechCon '22},
  selected  = {true}
}

@article{9591456,
  author    = {Huang, Sitao and Wu, Kun and Jeong, Hyunmin and Wang, Chengyue and Chen, Deming and Hwu, Wen-Mei},
  journal   = {IEEE Transactions on Computers},
  title     = {PyLog: An Algorithm-Centric Python-Based FPGA Programming and Synthesis Flow},
  paperlink = {https://ieeexplore.ieee.org/abstract/document/9591456},
  year      = {2021},
  volume    = {70},
  number    = {12},
  pages     = {2015-2028},
  doi       = {10.1109/TC.2021.3123465},
  codelink  = {https://github.com/hst10/pylog},
  abstract  = {The exploding complexity and computation efficiency requirements of applications are stimulating a strong demand for hardware acceleration with heterogeneous platforms such as FPGAs. However, a high-quality FPGA design is very hard to create and optimize as it requires FPGA expertise and a long design iteration time. In contrast, software applications are typically developed in a short development cycle, with high-level languages like Python, which have much higher levels of abstraction than all existing hardware design flows. To close this gap between hardware design flows and software applications, and simplify FPGA programming, we create PyLog, a high-level, algorithm-centric Python-based programming and synthesis flow for FPGA. PyLog is powered by a set of compiler optimization passes and a type inference system to generate high-quality hardware design. It abstracts away the implementation details, and allows designers to focus on algorithm specification. PyLog takes in Python functions, generates PyLog intermediate representation (PyLog IR), performs several optimization passes, including pragma insertion, design space exploration, and memory customization, etc., and creates complete FPGA system designs. PyLog also has a runtime that allows users to run the PyLog code directly on the target FPGA platform without any extra code development. The whole design flow is automated. Evaluation shows that PyLog significantly improves FPGA design productivity and generates highly efficient FPGA designs that outperform highly optimized CPU implementation and state-of-the-art FPGA implementation by 3.17× and 1.24× on average.},
  selected  = {true}
}


@article{10.14778/3476249.3476264,
  author     = {Min, Seung Won and Wu, Kun and Huang, Sitao and Hidayeto\u{g}lu, Mert and Xiong, Jinjun and Ebrahimi, Eiman and Chen, Deming and Hwu, Wen-mei},
  title      = {Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture},
  year       = {2021},
  issue_date = {July 2021},
  publisher  = {VLDB Endowment},
  volume     = {14},
  number     = {11},
  issn       = {2150-8097},
  url        = {https://doi.org/10.14778/3476249.3476264},
  paperlink  = {https://doi.org/10.14778/3476249.3476264},
  my_note    = {Upstreamed to DGL.},
  doi        = {10.14778/3476249.3476264},
  abstract   = {Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale graph-based recommender systems. Training GCN requires the minibatch generator traversing graphs and sampling the sparsely located neighboring nodes to obtain their features. Since real-world graphs often exceed the capacity of GPU memory, current GCN training systems keep the feature table in host memory and rely on the CPU to collect sparse features before sending them to the GPUs. This approach, however, puts tremendous pressure on host memory bandwidth and the CPU. This is because the CPU needs to (1) read sparse features from memory, (2) write features into memory as a dense format, and (3) transfer the features from memory to the GPUs.In this work, we propose a novel GPU-oriented data communication approach for GCN training, where GPU threads directly access sparse features in host memory through zero-copy accesses without much CPU help. By removing the CPU gathering stage, our method significantly reduces the consumption of the host resources and data access latency. We further present two important techniques to achieve high host memory access efficiency by the GPU: (1) automatic data access address alignment to maximize PCIe packet efficiency, and (2) asynchronous zero-copy access and kernel execution to fully overlap data transfer with training. We incorporate our method into PyTorch and evaluate its effectiveness using several graphs with sizes up to 111 million nodes and 1.6 billion edges. In a multi-GPU training setup, our method is 65--92% faster than the conventional data transfer method, and can even match the performance of all-in-GPU-memory training for some graphs that fit in GPU memory.},
  journal    = {Proceedings of the VLDB Endowment},
  pages      = {2087–2100},
  numpages   = {14},
  codelink   = {https://github.com/K-Wu/pytorch-direct_dgl},
  selected   = {true}
}

@inproceedings{min2021pytorchdirect,
  title         = {PyTorch-Direct: Enabling GPU Centric Data Access for Very Large Graph Neural Network Training with Irregular Accesses},
  author        = {Seung Won Min and Kun Wu and Sitao Huang and Mert Hidayetoğlu and Jinjun Xiong and Eiman Ebrahimi and Deming Chen and Wen-mei Hwu},
  year          = {2021},
  booktitle     = {arXiv preprint},
  arxiv         = {2101.07956},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {With the increasing adoption of graph neural networks (GNNs) in the machine learning community, GPUs have become an essential tool to accelerate GNN training. However, training GNNs on very large graphs that do not fit in GPU memory is still a challenging task. Unlike conventional neural networks, mini-batching input samples in GNNs requires complicated tasks such as traversing neighboring nodes and gathering their feature values. While this process accounts for a significant portion of the training time, we find existing GNN implementations using popular deep neural network (DNN) libraries such as PyTorch are limited to a CPU-centric approach for the entire data preparation step. This "all-in-CPU" approach has negative impact on the overall GNN training performance as it over-utilizes CPU resources and hinders GPU acceleration of GNN training. To overcome such limitations, we introduce PyTorch-Direct, which enables a GPU-centric data accessing paradigm for GNN training. In PyTorch-Direct, GPUs are capable of efficiently accessing complicated data structures in host memory directly without CPU intervention. Our microbenchmark and end-to-end GNN training results show that PyTorch-Direct reduces data transfer time by 47.1% on average and speeds up GNN training by up to 1.6x. Furthermore, by reducing CPU utilization, PyTorch-Direct also saves system power by 12.4% to 17.5% during training. To minimize programmer effort, we introduce a new "unified tensor" type along with necessary changes to the PyTorch memory allocator, dispatch logic, and placement rules. As a result, users need to change at most two lines of their PyTorch GNN training code for each tensor object to take advantage of PyTorch-Direct.},
  codelink      = {https://github.com/K-Wu/pytorch-direct_dgl},
  selected      = {false}
}

@inproceedings{9651170,
  author    = {Huang, Sitao and Wu, Kun and Chalamalasetti, Sai Rahul and El Hajj, Izzat and Xu, Cong and Faraboschi, Paolo and Chen, Deming},
  booktitle = {2021 IEEE/ACM Programming Environments for Heterogeneous Computing (PEHC)},
  title     = {A Python-based High-Level Programming Flow for CPU-FPGA Heterogeneous Systems : (Invited Paper)},
  year      = {2021},
  paperlink = {https://ieeexplore.ieee.org/abstract/document/9651170},
  volume    = {},
  number    = {},
  pages     = {20-26},
  codelink  = {https://github.com/hst10/pylog},
  doi       = {10.1109/PEHC54839.2021.00008}
}

@inproceedings{10.1145/3431379.3460645,
  author    = {Pearson, Carl and Wu, Kun and Chung, I-Hsin and Xiong, Jinjun and Hwu, Wen-Mei},
  title     = {TEMPI: An Interposed MPI Library with a Canonical Representation of CUDA-Aware Datatypes},
  year      = {2021},
  isbn      = {9781450382175},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3431379.3460645},
  paperlink = {https://doi.org/10.1145/3431379.3460645},
  doi       = {10.1145/3431379.3460645},
  abstract  = {MPI derived datatypes are an abstraction that simplifies handling of non-contiguous data in MPI applications. These datatypes are recursively constructed at runtime from primitive Named Types defined in the MPI standard. More recently, the development and deployment of CUDA-aware MPI implementations has encouraged the transition of distributed high-performance MPI codes to use GPUs. Such implementations allow MPI functions to directly operate on GPU buffers, easing integration of GPU compute into MPI codes. This work first presents a novel datatype handling strategy for nested strided datatypes, which finds a middle ground between the specialized or generic handling in prior work. This work also shows that the performance characteristics of non-contiguous data handling can be modeled with empirical system measurements, and used to transparently improve MPI_Send/Recv latency. Finally, despite substantial attention to non-contiguous GPU data and CUDA-aware MPI implementations, good performance cannot be taken for granted. This work demonstrates its contributions through an MPI interposer library, TEMPI. TEMPI can be used with existing MPI deployments without system or application changes. Ultimately, the interposed-library model of this work demonstrates MPI_Pack speedup of up to 242000x and MPI_Send speedup of up to 59000x compared to the MPI implementation deployed on a leadership-class supercomputer. This yields speedup of more than 917x in a 3D halo exchange with 3072 processes.},
  booktitle = {Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing},
  pages     = {95–106},
  numpages  = {12},
  keywords  = {summit, cuda, derived datatype, spectrum mpi, mpi},
  location  = {Virtual Event, Sweden},
  series    = {HPDC '21},
  codelink  = {https://github.com/cwpearson/tempi},
  selected  = {true}
}

@inproceedings{10.1145/3316781.3317862,
  author    = {Wu, Kun and Dai, Guohao and Hu, Xing and Li, Shuangchen and Xie, Xinfeng and Wang, Yu and Xie, Yuan},
  title     = {Memory-Bound Proof-of-Work Acceleration for Blockchain Applications},
  year      = {2019},
  isbn      = {9781450367257},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3316781.3317862},
  doi       = {10.1145/3316781.3317862},
  abstract  = {Blockchain applications have shown huge potential in various domains. Proof of Work (PoW) is the key procedure in blockchain applications, which exhibits the memory-bound characteristic and hinders the performance improvement of blockchain accelerators. In order to mitigate the "memory wall" and improve the performance of memory-hard PoW accelerators, using Ethash as an example, we optimize the memory architecture from two perspectives: 1) Hiding memory latency. We propose specialized context switch design to overcome the uncertain cycles of repetitive memory requests. 2) Increasing memory bandwidth utilization. We introduce on-chip memory that stores a portion of the Ethash directed acyclic graph (DAG) for larger effective memory bandwidth, and further propose adopting embedded NOR flash to fulfill the role. Then, we conduct extensive experiments to explore the design space of our optimized memory architecture for Ethash, including number of hash cores, on-chip/off-chip memory technologies and specifications. Based on the design space exploration, we finally provide the guidance for designing the memory-bound PoW accelerator. The experiment results show that our optimized designs achieve 8.7% -- 55% higher hash rate and 17% -- 120% higher hash rate per Joule compared with the baseline design in different configurations.},
  booktitle = {Proceedings of the 56th Annual Design Automation Conference},
  articleno = {177},
  numpages  = {6},
  location  = {Las Vegas, NV, USA},
  series    = {DAC '19},
  selected  = {true}
}